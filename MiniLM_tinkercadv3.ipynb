{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf0dd01-df91-455d-b6b9-74e6cbeba141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pdfplumber\n",
    "import time\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from tkinter import scrolledtext  # UPDATE: Fixed missing import\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2edf5137-2361-4c59-b7b5-e3a6145e9d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SentenceTransformer model\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Global variables\n",
    "DATASET_PATH = \"\"\n",
    "EMBEDDINGS_FILE = \"document_embeddings.npy\"\n",
    "FAISS_FILE = \"faiss_index.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "307135fc-0677-4886-8a7d-d20a21e7bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_documents(documents):\n",
    "    return embedding_model.encode(documents, convert_to_numpy=True)\n",
    "\n",
    "def generate_query_embedding(query):\n",
    "    return embedding_model.encode(query, convert_to_numpy=True)\n",
    "\n",
    "def create_faiss_index(embeddings, embedding_dim):\n",
    "    index = faiss.IndexFlatL2(embedding_dim)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def save_embeddings(embeddings, file_path):\n",
    "    np.save(file_path, embeddings)\n",
    "\n",
    "def load_embeddings(file_path):\n",
    "    return np.load(file_path)\n",
    "\n",
    "def save_faiss_index(index, file_path):\n",
    "    faiss.write_index(index, file_path)\n",
    "\n",
    "def load_faiss_index(file_path):\n",
    "    return faiss.read_index(file_path)\n",
    "\n",
    "def read_pdfs_from_folder(folder_path):\n",
    "    pdf_contents = []\n",
    "    pdf_filenames = []\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"Dataset folder '{folder_path}' does not exist.\")\n",
    "    \n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(f\"No PDF files found in '{folder_path}'.\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        file_path = os.path.join(folder_path, pdf_file)\n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                text = \"\".join([page.extract_text() or \"\" for page in pdf.pages])\n",
    "                if text.strip():\n",
    "                    pdf_contents.append(text)\n",
    "                    pdf_filenames.append(pdf_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF file {pdf_file}: {e}\")\n",
    "    \n",
    "    return pdf_contents, pdf_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4922386-10a2-458c-94d9-b4dcb192c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with_nltk_focused(document, query, max_lines=20):\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from string import punctuation\n",
    "    from heapq import nlargest\n",
    "    \n",
    "    sentences = sent_tokenize(document)\n",
    "    english_sentences = [s for s in sentences if len(s.strip()) > 10 and detect(s) == 'en']\n",
    "    if not english_sentences:\n",
    "        return \"No relevant English content found related to the query.\"\n",
    "    \n",
    "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "    query_terms = [term.lower() for term in query.split() if term.lower() not in stop_words]\n",
    "    \n",
    "    sentence_scores = {}\n",
    "    for sentence in english_sentences:\n",
    "        score = sum(1 for word in word_tokenize(sentence.lower()) if word in query_terms)\n",
    "        if score > 0:\n",
    "            sentence_scores[sentence] = score\n",
    "    \n",
    "    summary_sentences = nlargest(min(len(sentence_scores), max_lines), sentence_scores, key=sentence_scores.get)\n",
    "    return ' '.join(summary_sentences) if summary_sentences else f\"No content specifically about '{query}' found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c76d7d-0591-4d10-83ee-8a1e76878755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPDATE: Adjusted output display to use the full window\n",
    "def process_documents(documents, query, num_docs=2):\n",
    "    \"\"\"Process documents and return summaries based on the query.\"\"\"\n",
    "    final_output = []\n",
    "    for doc_idx, document in enumerate(documents[:num_docs]):\n",
    "        summary = summarize_with_nltk_focused(document, query)\n",
    "        if summary and not summary.startswith(\"No content\"):\n",
    "            final_output.append(f\"Document {doc_idx + 1} Response:\\n{summary}\")\n",
    "    \n",
    "    if not final_output:\n",
    "        return f\"No relevant information found about '{query}' in the documents.\"\n",
    "    \n",
    "    return \"\\n\\n\".join(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee2d90de-10c9-4016-b58e-142350de7fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query():\n",
    "    global DATASET_PATH\n",
    "    query = query_entry.get()\n",
    "    if not query:\n",
    "        messagebox.showerror(\"Error\", \"Please enter a query\")\n",
    "        return\n",
    "    if not DATASET_PATH:\n",
    "        messagebox.showerror(\"Error\", \"Please select a dataset folder\")\n",
    "        return\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response_text.delete(1.0, tk.END)\n",
    "    response_text.insert(tk.END, \"Processing...\\n\")\n",
    "    root.update()\n",
    "    \n",
    "    try:\n",
    "        documents, filenames = read_pdfs_from_folder(DATASET_PATH)\n",
    "        if os.path.exists(EMBEDDINGS_FILE):\n",
    "            document_embeddings = load_embeddings(EMBEDDINGS_FILE)\n",
    "        else:\n",
    "            document_embeddings = embed_documents(documents)\n",
    "            save_embeddings(document_embeddings, EMBEDDINGS_FILE)\n",
    "        \n",
    "        if os.path.exists(FAISS_FILE):\n",
    "            faiss_index = load_faiss_index(FAISS_FILE)\n",
    "        else:\n",
    "            embedding_dim = document_embeddings.shape[1]\n",
    "            faiss_index = create_faiss_index(document_embeddings, embedding_dim=embedding_dim)\n",
    "            save_faiss_index(faiss_index, FAISS_FILE)\n",
    "        \n",
    "        query_embedding = generate_query_embedding(query).reshape(1, -1)\n",
    "        distances, indices = faiss_index.search(query_embedding, k=2)\n",
    "        top_documents = [documents[idx] for idx in indices[0] if idx < len(filenames)]\n",
    "        \n",
    "        final_output = process_documents(top_documents, query) if top_documents else \"No relevant documents found.\"\n",
    "    except Exception as e:\n",
    "        final_output = f\"Error: {str(e)}\"\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    response_text.delete(1.0, tk.END)\n",
    "    response_text.insert(tk.END, final_output)\n",
    "    timer_label.config(text=f\"Time taken: {elapsed_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ed33cf9-b410-4b40-b02f-72cb42cf0a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_folder():\n",
    "    global DATASET_PATH\n",
    "    DATASET_PATH = filedialog.askdirectory()\n",
    "    folder_label.config(text=f\"Selected Folder: {DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcda4078-57e0-4ed9-8b70-628baab053be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUI Setup\n",
    "root = tk.Tk()\n",
    "root.title(\"NLP Query Interface\")\n",
    "root.geometry(\"800x600\")\n",
    "\n",
    "folder_button = tk.Button(root, text=\"Select Dataset Folder\", command=select_folder)\n",
    "folder_button.pack(pady=5)\n",
    "\n",
    "folder_label = tk.Label(root, text=\"No folder selected\", wraplength=700)\n",
    "folder_label.pack()\n",
    "\n",
    "query_entry = tk.Entry(root, width=70)\n",
    "query_entry.pack(pady=5)\n",
    "query_entry.insert(0, \"Enter your query here\")\n",
    "\n",
    "submit_button = tk.Button(root, text=\"Submit Query\", command=process_query)\n",
    "submit_button.pack(pady=5)\n",
    "\n",
    "response_text = scrolledtext.ScrolledText(root, wrap=tk.WORD, width=100, height=20)\n",
    "response_text.pack(expand=True, fill=tk.BOTH, padx=10, pady=10)\n",
    "\n",
    "timer_label = tk.Label(root, text=\"\")\n",
    "timer_label.pack()\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20254fff-7a07-4cf6-b92d-24055950fe59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
